{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>hi</p>"},{"location":"perceptron/","title":"Perceptron","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\n</pre> import numpy as np import pandas as pd In\u00a0[2]: Copied! <pre>def prepare_data(csv):\n    # Getting data ready\n    data = pd.read_csv(csv)\n\n    # Data normalisation\n    data['Age'] = data['Age'] / 100\n\n    # z-score normalisation\n    data['Income'] = (data['Income'] - data['Income'].mean()) / data['Income'].std()\n    data['Previous Purchases'] = (data['Previous Purchases'] - data['Previous Purchases'].mean()) / data['Previous Purchases'].std()\n\n    return data\n</pre> def prepare_data(csv):     # Getting data ready     data = pd.read_csv(csv)      # Data normalisation     data['Age'] = data['Age'] / 100      # z-score normalisation     data['Income'] = (data['Income'] - data['Income'].mean()) / data['Income'].std()     data['Previous Purchases'] = (data['Previous Purchases'] - data['Previous Purchases'].mean()) / data['Previous Purchases'].std()      return data In\u00a0[3]: Copied! <pre>def prepare_matrices(data):\n    # Prepare input matrix and prediction matrix\n    X = []\n    Y = []\n    for idx, rows in data.iterrows():\n        x, y = rows.iloc[:-1].to_numpy(), rows.iloc[-1]\n        X.append(x)\n        Y.append(y)\n\n    X = np.array(X)\n    Y = np.array(Y)\n\n    return X, Y\n</pre> def prepare_matrices(data):     # Prepare input matrix and prediction matrix     X = []     Y = []     for idx, rows in data.iterrows():         x, y = rows.iloc[:-1].to_numpy(), rows.iloc[-1]         X.append(x)         Y.append(y)      X = np.array(X)     Y = np.array(Y)      return X, Y In\u00a0[4]: Copied! <pre>def get_weights_bias(input_size):\n    # Init weights\n    W = np.random.rand(input_size)\n\n    # init weights\n    b = np.random.rand(1)\n\n    return W, b\n</pre> def get_weights_bias(input_size):     # Init weights     W = np.random.rand(input_size)      # init weights     b = np.random.rand(1)      return W, b In\u00a0[7]: Copied! <pre>data = prepare_data('../data/purchase_prediction.csv')\nX, Y = prepare_matrices(data)\nW, b = get_weights_bias(X.shape[1])\n</pre> data = prepare_data('../data/purchase_prediction.csv') X, Y = prepare_matrices(data) W, b = get_weights_bias(X.shape[1]) In\u00a0[8]: Copied! <pre># Sigmoid activation\ndef sigmoid(H):\n    return 1 / (1 + np.exp(-H))\n\n# BCE \ndef cross_entropy(y, y_pred):\n    y_pred = np.clip(y_pred, 1e-9, 1 - 1e-9)\n    return -np.sum(y*np.log(y_pred) + (1-y)*np.log(1-y_pred)) / len(y)\n\n# BCE grad\ndef cross_entropy_grad(y, y_pred):\n    return -np.mean((y/y_pred) + ((1-y)/(1-y_pred)))\n</pre> # Sigmoid activation def sigmoid(H):     return 1 / (1 + np.exp(-H))  # BCE  def cross_entropy(y, y_pred):     y_pred = np.clip(y_pred, 1e-9, 1 - 1e-9)     return -np.sum(y*np.log(y_pred) + (1-y)*np.log(1-y_pred)) / len(y)  # BCE grad def cross_entropy_grad(y, y_pred):     return -np.mean((y/y_pred) + ((1-y)/(1-y_pred))) In\u00a0[9]: Copied! <pre>epochs = 100000\nlr = 0.01\nprint_after = 10000\n\nfor epoch in range(1, epochs+1):\n    ###\n    # feed forward\n    # sum\n    H = np.matmul(X, W) + b\n\n    # activation\n    A = sigmoid(H)\n\n    # calc binary cross entropy loss\n    loss = cross_entropy(Y, A)\n\n    ###\n    # backpropagate\n    dz = A - Y\n    dw = (1/len(Y)) * np.matmul(X.T, dz)\n    db = (1/len(Y)) * np.sum(dz)\n\n    # update weights\n    W = W - lr*dw\n    b = b - lr*db\n\n    if epoch % print_after == 0:\n        print(f\"Epoch: {epoch}/{epochs}| Loss: {loss}\")\n</pre> epochs = 100000 lr = 0.01 print_after = 10000  for epoch in range(1, epochs+1):     ###     # feed forward     # sum     H = np.matmul(X, W) + b      # activation     A = sigmoid(H)      # calc binary cross entropy loss     loss = cross_entropy(Y, A)      ###     # backpropagate     dz = A - Y     dw = (1/len(Y)) * np.matmul(X.T, dz)     db = (1/len(Y)) * np.sum(dz)      # update weights     W = W - lr*dw     b = b - lr*db      if epoch % print_after == 0:         print(f\"Epoch: {epoch}/{epochs}| Loss: {loss}\") <pre>Epoch: 10000/100000| Loss: 0.14486881031094775\nEpoch: 20000/100000| Loss: 0.12188557399732414\nEpoch: 30000/100000| Loss: 0.10799112930053398\nEpoch: 40000/100000| Loss: 0.09740161380919043\nEpoch: 50000/100000| Loss: 0.08875526442831948\nEpoch: 60000/100000| Loss: 0.08147343673857062\nEpoch: 70000/100000| Loss: 0.0752295171311487\nEpoch: 80000/100000| Loss: 0.06980930346019368\nEpoch: 90000/100000| Loss: 0.06505985371220571\nEpoch: 100000/100000| Loss: 0.060866222059249894\n</pre> In\u00a0[11]: Copied! <pre>x_1 = np.array([0.32, 0.671181, 0.493606])\nx_2 = np.array([0.48, -0.979264, -0.211545])\ndef predict(input_x):\n    H = np.matmul(input_x, W)\n    A = sigmoid(H)\n    print(A)\n    if A &gt; 0.5:\n        print(1)\n    else:\n        print(0)\n\npredict(x_1) # 1\npredict(x_2) # 0\n</pre> x_1 = np.array([0.32, 0.671181, 0.493606]) x_2 = np.array([0.48, -0.979264, -0.211545]) def predict(input_x):     H = np.matmul(input_x, W)     A = sigmoid(H)     print(A)     if A &gt; 0.5:         print(1)     else:         print(0)  predict(x_1) # 1 predict(x_2) # 0 <pre>0.9968600634790575\n1\n0.0006271349965844889\n0\n</pre> In\u00a0[12]: Copied! <pre>data\n</pre> data Out[12]: Age Income Previous Purchases Purchased 0 0.25 -0.429116 -0.916696 0 1 0.32 0.671181 0.493606 1 2 0.48 -0.979264 -0.211545 0 3 0.18 -1.254339 -0.916696 0 4 0.65 1.771478 1.198756 1 5 0.42 0.396107 -0.916696 0 6 0.28 -0.539146 -0.211545 1 7 0.55 1.221330 1.903907 1 8 0.35 0.011003 0.493606 1 9 0.22 -0.869235 -0.916696 0 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"perceptron/#perceptron","title":"Perceptron\u00b6","text":""},{"location":"perceptron/#1-data-preparation","title":"1. Data preparation\u00b6","text":""},{"location":"perceptron/#2-setting-up-the-perceptron","title":"2. Setting up the perceptron\u00b6","text":""},{"location":"perceptron/#3-training","title":"3. Training\u00b6","text":""},{"location":"perceptron/#testing","title":"Testing\u00b6","text":""}]}